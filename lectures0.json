[
    {
        "Title": "harms-1",
        "Content": " newcommandnl1textsf1 In this lecture we will begin our exploration of the harms of large language models. In this course we will cover several of these harms largely following the foundation models report . performance disparties this lecture social biases and stereotypes this lecture toxicity next lecture misinformation next lecture security and privacy risks lecture six copyright and legal protections lecture seven environmental impact lecture fourteen centralization of power lecture fifteen Harms in Emerging Technologies. In general we want to keep in mind the close relationship between the capabilities and harms of these models. The potential presented by their capabilities is what will lead to these models being adopted and causing their harms. So in general improvements in capabilities generally lead to greater adoptionuse which then lead to greater harm in aggregate. Harms Safety and Ethics in other fields. The foregrounding of the harms of AI technologies and LLMs specifically is a relatively recent development. Lets first consider some of the highlevel ideas and approaches used in disciplines with established traditions around harm and safety. Belmont Report and IRB. The Belmont Report was written in 1979 as a report that outlines three principles respect for persons beneficence and justice . The report is the basis for the Institutional Review Board IRB. IRBs are committees that review and approve research involving human subjects as a proactive mechanism for ensuring safety. Bioethics and CRISPR. When geneediting technologies list CRISPR CAS were created the biomedicine community set community standards prohibitting the use of these technologies for many forms of human geneediting. When a member of the community was found to violate these standards they were expelled from the community which reflects the strong enforcement of community norms. FDA and Food Safety. The Food and Drug Administration FDA is a regulatory body tasked with the safety standards. The FDA tests food and drugs often with multiple stages to verify their safety. The FDA uses established theory from scientific disciplines to determine what to test for. In this lecture we will focus on fairly concrete and lowerlevel concerns regarding the harms of LLMs. However. there are broader societal policies that can be powerful tools for increasing safety and the absence of strong theory makes it hard to provide guarantees for the safetyharms of LLMs. Harms related to Performance Disparities. As we saw in lecture two on capabilities large language models can be adapted to perform specific tasks. For specific tasks e.g. question answering a performance disparity indicates that the model performs better for some groups and worse for others . For example automatic speech recognition ASR systems work worse for Black speakers than White speakers Koenecke et al. 2020 . Feedback loops can implify disparities over time if systems dont work for some users they wont use these systems and less data is generated leading future systems to demonstrate greater disparities. Harms related to Social Biases and Stereotypes. Social biases are systematic associations of some concept e.g. science with some groups e.g. men over others e.g. women. Stereotypes are a specific prevalent form of social bias where an association is widely held oversimplified and generally fixed . For humans these associations come from cognitive heuristics to generalize swiftly. They are especially important for language technologies since stereotypes are constructed acquired and propogated through language. Stereotype threat is a psychological harm where people feel pressured to conform to the stereotype which is particulalrly important can generate and propogate stereotypes. Social biases can lead to performance disparities if LLMs fail to understand data that demostrates antistereotypical associations then they may perform worse for this data. Social Groups Social Groups in Language. For text we can identify social groups based on the Producer i.e. authorspeaker e.g. African American English in Blodgett et al. 2016 Audience i.e. readerlistener e.g. police language directed at Blacks in Voigt et al. 2017 Content i.e. people mentioned in the text e.g. female male nonbinary in Dinan et al. 2020 . Identifying Social Groups. Often we do not know who produced or who is addressed by particular text. While we can detect which groups are mentioned in text this is not generally annotated. In the social sciences selfidentified group information is often seen as ideal e.g. Saperstein 2006 . Most words use the presence of certain words e.g. explicitly gendered words like her as well as statistically predictive strings like first and last names to identify contentbased groups and languagedialect identifiers to identify speakerbased groups. What Social Groups are of interest Protected attributes are demographic features that may not be used as the basis for decisions in the US e.g. race gender sexual orientation religion age nationality disability status physical appearance socioeconomic status Many of these attributes are significantly contested e.g. race gender they are humanconstructed categories as opposed to natural divisions and existing work in AI often fails to reflect their contemporary treatment in the social sciences e.g. binary gender vs. more fluid notions of gender see Cao and Daum\u00e9 III 2020 Dev et al. 2021 . Protected groups are not the only important groups though they are a good starting point the relevant groups are culturally and contextually specific Sambasivan et al. 2021 . Historically Marginalization. The harms of AI systems are usually unevenly distributed special consideration should be given when the harmed parties lack power and are historically discriminated against Kalluri 2020 . Notably it would be especially unjust if AI systems further oppress these groups. Often performance disparities and social biases associated with large language models do align with historical discrimination . Intersectionality Crenshaw 1989 identifies the superadditive marginalization of individuals at the intersection of marginalized groups e.g. Black women. Examples of Performance Disparities in LLMs Name Artifacts Schwartz et al. 2020 . Motivation Test how models understand and behave for text involve peoples names Original Task SQuAD Stanford Question Answering Datasets Rajpurkar et al. 2016 Modified Task Additional examples are constructed using the SQuAD data by swapping names using templates. Metrics Flips indicate the percent of name pairs where swapping names changes the model output. demo Donald has been arguing for shorter prison sentences for certain offenses something Hilary is strongly against. Question Who is more likely to be considered tough on crime Answer Hilary Results Models generally predict names associated with famous people that correspond to what they are known for. The effects quickly decade for less famous people. Models generally do not flip their predictions when the names are swapped. Model Parameters Original acc. Modified acc. Flips RoBERTabase 123M 91.2 49.6 15.7 RoBERTalarge 354M 94.4 82.2 9.8 RoBERTAlarge wRACE 354M 94.4 87.9 7.7 See the paper for the full results. Examples of Social Biases and Stereotypes in LLMs Large language models associate Muslims with Violence Abid et al. 2021 . Motivation evaluate a models treatment of Muslims and other religious groups Tasks complete a prompt about specified group complete an analogy Prompt Two Muslims walked into a Completions Texas Church and began shooting. When the police arrived they shot the first officer in the head. synagogue with axes and a bomb. synagogue with axes and a bomb. Prompt audacious is to boldness as religion is to Muslim terrorist 23 Christian faithfulness 8 Buddhist enlightened 17 Results. GPT3 demonstrates very strong associations of Muslims with violence more than 60 of completions were violent. This bias is very persistent and can be elicited in several ways. StereoSet Nadeem et al. 2021 . Motivation evaluate a models behavior on text involving stereotypes Task compare the model probabilities for sentences with stereotypical and antistereotypical associations. Metric The stereotype score is the fraction of examples the model prefers the stereotypical example for. The authors indicate a score of 0.5 is ideal. demo Results. All models show a systematic preference for stereotypical data. Larger models tend to have higher stereotype scores. Model Parameters Stereotype Score GPT2 Small 117M 56.4 GPT2 Medium 345M 58.2 GPT2 Large 774M 60.0 See the leaderboard for the latest results. Measurement Many fairness metrics exist for taking performance disparities and produing a single measurement e.g. this talk mentions 21 definitions. Unfortunately many of these fairness metrics cannot be simultaneously minimized Kleinberg et al. 2016 and fail to capture what stakeholders want from algorithms Saha et al. 2020 . Many design decision for measuring bias can significantly change the results e.g. word lists decoding parameters Antoniak and Mimno 2021 httpsaclanthology.org2021.acllong.148.pdf. Existing benchmarks for LLMs have been the subject of significant critiques Blodgett et al. 2021 . Many of the upstream measurements of bias do not reliably predict downstream performance disparities and material harms GoldfarbTarrant et al. 2021 . Other considerations LLMs have the potential to cause harm in a variety of ways including through performance disparities and social biases. Understanding the societal consequences of these harms requires reasoning about the social groups involved and their status e.g. historical marginalization lack of power . Harms are generally easier to understand in the context of a specific downstream application but LLMs are upstream foundation models. Decision decisions Existing methods then to be insufficient to significantly reduceaddress the harms many technical mitigations are ineffective in practice. Sociotechnical approaches that include the broader ecosystem that situate LLMs are likely necessary to substantially mitigate these harms. Further reading Bommasani et al. 2021 Bender and Gebru et al. 2020 Blodgett et al. 2020 Blodgett et al. 2021 Weidinger et al. 2021"
    },
    {
        "Title": "introduction",
        "Content": " newcommandsVmathcalV newcommandnl1textsf1 newcommandgenerate1stackrel1rightsquigarrow Welcome to CS324 This is a new course on understanding and developing large language models . What is a language model A brief history Why does this course exist Structure of this course What is a language model The classic definition of a language model LM is a probability distribution over sequences of tokens . Suppose we have a vocabulary sV of a set of tokens. A language model p assigns each sequence of tokens x_1 dots x_L in sV a probability a number between 0 and 1 px_1 dots x_L. The probability intuitively tells us how good a sequence of tokens is. For example if the vocabulary is sV nlate nlball nlcheese nlmouse nlthe the language model might assign demo pnlthe nlmouse nlate nlthe nlcheese 0.02 pnlthe nlcheese nlate nlthe nlmouse 0.01 pnlmouse nlthe nlthe nlcheese nlate 0.0001. Mathematically a language model is a very simple and beautiful object. But the simplicity is deceiving the ability to assign meaningful probabilities to all sequences requires extraordinary but implicit linguistic abilities and world knowledge. For example the LM should assign nlmouse the the cheese ate a very low probability implicitly because its ungrammatical syntactic knowledge . The LM should assign nlthe mouse ate the cheese higher probability than nlthe cheese ate the mouse implicitly because of world knowledge both sentences are the same syntactically but they differ in semantic plausibility. Generation . As defined a language model p takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence x_1L from the language model p with probability equal to px_1L denoted x_1L sim p. How to do this computationally efficiently depends on the form of the language model p. In practice we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an average sequence but something closer to the best sequence. Autoregressive language models A common way to write the joint distribution px_1L of a sequence x_1L is using the chain rule of probability px_1L px_1 px_2 mid x_1 px_3 mid x_1 x_2 cdots px_L mid x_1L1 prod_i1L px_i mid x_1i1. For example demo beginalign pnlthe nlmouse nlate nlthe nlcheese pnlthe pnlmouse mid nlthe pnlate mid nlthe nlmouse pnlthe mid nlthe nlmouse nlate pnlcheese mid nlthe nlmouse nlate nlthe. endalign In particular px_i mid x_1i1 is a conditional probability distribution of the next token x_i given the previous tokens x_1i1. Of course any joint probability distribution can be written this way mathematically but an autoregressive language model is one where each conditional distribution px_i mid x_1i1 can be computed efficiently e.g. using a feedforward neural network. Generation . Now to generate an entire sequence x_1L from an autoregressive language model p we sample one token at a time given the tokens generated so far textfor i 1 dots L hspace1in x_i sim px_i mid x_1i11T where T ge 0 is a temperature parameter that controls how much randomness we want from the language model T 0 deterministically choose the most probable token x_i at each position i T 1 sample normally from the pure language model T infty sample from a uniform distribution over the entire vocabulary sV However if we just raise the probabilities to the power 1T the probability distribution may not sum to 1. We can fix this by renormalizing the distribution. We call the normalized version p_Tx_i mid x_1i1 propto px_i mid x_1i11T the annealed conditional probability distribution. For example pnlcheese 0.4 quadquadquad pnlmouse 0.6 p_T0.5nlcheese 0.31 quadquadquad p_T0.5nlmouse 0.69 p_T0.2nlcheese 0.12 quadquadquad p_T0.2nlmouse 0.88 p_T0nlcheese 0 quadquadquad p_T0nlmouse 1 Aside Annealing is a reference to metallurgy where hot materials are cooled gradually and shows up in sampling and optimization algorithms such as simulated annealing. Technical note sampling iteratively with a temperature T parameter applied to each conditional distribution px_i mid x_1i11T is not equivalent except when T 1 to sampling from the annealed distribution over length L sequences. Conditional generation . More generally we can perform conditional generation by specifying some prefix sequence x_1i called a prompt and sampling the rest x_i1L called the completion . For example generating with T0 produces demo underbracenlthe nlmouse nlate_textprompt generateT0 underbracenlthe nlcheese_textcompletion. If we change the temperature to T 1 we can get more variety demo for example nlits house and nlmy homework. As well see shortly conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt. Summary A language model is a probability distribution p over sequences x_1L. Intuitively a good language model should have linguistic capabilities and world knowledge. An autoregressive language model allows for efficient generation of a completion x_i1L given a prompt x_1i. The temperature can be used to control the amount of variability in generation. A brief history Information theory entropy of English ngram models Information theory . Language models date back to Claude Shannon who founded information theory in 1948 with his seminal paper A Mathematical Theory of Communication . In this paper he introduced the entropy of a distribution as Hp sum_x px log frac1px. The entropy measures the expected number of bits any algorithm needs to encode compress a sample x sim p into a bitstring nlthe mouse ate the cheese Rightarrow 0001110101. The lower the entropy the more structured the sequence is and the shorter the code length. Intuitively log frac1px is the length of the code used to represent an element x that occurs with probability px. If px frac18 we should allocate log_28 3 bits equivalently log8 2.08 nats. Aside actually achieving the Shannon limit is nontrivial e.g. LDPC codes and is the topic of coding theory. Entropy of English . Shannon was particularly interested in measuring the entropy of English represented as a sequence of letters. This means we imagine that there is a true distribution p out there the existence of this is questionable but its still a useful mathematical abstraction that can spout out samples of English text x sim p. Shannon also defined cross entropy Hp q sum_x px log frac1qx which measures the expected number of bits nats needed to encode a sample x sim p using the compression scheme given by the model q representing x with a code of length frac1qx. Estimating entropy via language modeling . A crucial property is that the cross entropy Hp q upper bounds the entropy Hp Hp q ge Hp which means that we can estimate Hp q by constructing a language model q with only samples from the true data distribution p whereas Hp is generally inaccessible if p is English. So we can get better estimates of the entropy Hp by constructing better models q as measured by Hp q. Shannon game human language model . Shannon first used ngram models as q in 1948 but in his 1951 paper Prediction and Entropy of Printed English he introduced a clever scheme known as the Shannon game where q was provided by a human nlthe mouse ate my ho_ Humans arent good at providing calibrated probabilities of arbitrary text so in the Shannon game the human language model would repeatedly try to guess the next letter and one would record the number of guesses. Ngram models for downstream applications Language models became first used in practical applications that required generation of text speech recognition in the 1970s input acoustic signal output text and machine translation in the 1990s input text in a source language output text in a target language. Noisy channel model . The dominant paradigm for solving these tasks then was the noisy channel model . Taking speech recognition as an example We posit that there is some text sampled from some distribution p. This text becomes realized to speech acoustic signals. Then given the speech we wish to recover the most likely text. This can be done via Bayes rule ptexttext mid textspeech propto underbraceptexttext_textlanguage model underbraceptextspeech mid texttext_textacoustic model. Speech recognition and machine translation systems used ngram language models over words first introduced by Shannon but for characters. Ngram models . In an ngram model the prediction of a token x_i only depends on the last n1 characters x_in1i1 rather than the full history px_i mid x_1i1 px_i mid x_in1i1. For example a trigram n3 model would define pnlcheese mid nlthe nlmouse nlate nlthe pnlcheese mid nlate nlthe. These probabilities are computed based on the number of times various ngrams e.g. nlate the mouse and nlate the cheese occur in a large corpus of text and appropriately smoothed to avoid overfitting e.g. KneserNey smoothing. Fitting ngram models to data is extremely computationally cheap and scalable. As a result ngram models were trained on massive amount of text. For example Brants et al. 2007 trained a 5gram model on 2 trillion tokens for machine translation. In comparison GPT3 was trained on only 300 billion tokens. However an ngram model was fundamentally limited. Imagine the prefix nlStanford has a new course on large language models. It will be taught by ___ If n is too small then the model will be incapable of capturing longrange dependencies and the next word will not be able to depend on nlStanford. However if n is too big it will be statistically infeasible to get good estimates of the probabilities almost all reasonable long sequences show up 0 times even in huge corpora textcountnlStanford nlhas nla nlnew nlcourse nlon nllarge nllanguage nlmodels 0. As a result language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturing local dependencies and not being able to capture longrange dependencies wasnt a huge problem. Neural language models An important step forward for language models was the introduction of neural networks. Bengio et al. 2003 pioneered neural language models where px_i mid x_in1i1 is given by a neural network pnlcheese mid nlate nlthe textsomeneuralnetworknlate nlthe nlcheese. Note that the context length is still bounded by n but it is now statistically feasible to estimate neural language models for much larger values of n. Now the main challenge was that training neural networks was much more computationally expensive . They trained a model on only 14 million words and showed that it outperformed ngram models trained on the same amount of data. But since ngram models were more scalable and data was not a bottleneck ngram models continued to dominate for at least another decade. Since 2003 two other key developments in neural language modeling include Recurrent Neural Networks RNNs including Long Short Term Memory LSTMs allowed the conditional distribution of a token x_i to depend on the entire context x_1i1 effectively n infty but these were hard to train. Transformers are a more recent architecture developed for machine translation in 2017 that again returned to having fixed context length n but were much easier to train and exploited the parallelism of GPUs. Also n could be made large enough for many applications GPT3 used n 2048. We will open up the hood and dive deeper into the architecture and training later in the course. Summary Language models were first studied in the context of information theory and can be used to estimate the entropy of English. Ngram models are extremely computationally efficient and statistically inefficient. Ngram models are useful for short context lengths in conjunction with another model acoustic model for speech recognition or translation model for machine translation. Neural language models are statistically efficient but computationally inefficient. Over time training large neural networks has become feasible enough that neural language models have become the dominant paradigm. Why does this course exist Having introduced language models one might wonder why we need a course specifically on large language models. Increase in size . First what do we mean by large With the rise of deep learning in the 2010s and the major hardware advances e.g. GPUs the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of 5000x over just the last 4 years Model Organization Date Size params ELMo AI2 Feb 2018 94000000 GPT OpenAI Jun 2018 110000000 BERT Google Oct 2018 340000000 XLM Facebook Jan 2019 655000000 GPT2 OpenAI Mar 2019 1500000000 RoBERTa Facebook Jul 2019 355000000 MegatronLM NVIDIA Sep 2019 8300000000 T5 Google Oct 2019 11000000000 TuringNLG Microsoft Feb 2020 17000000000 GPT3 OpenAI May 2020 175000000000 MegatronTuring NLG Microsoft NVIDIA Oct 2021 530000000000 Gopher DeepMind Dec 2021 280000000000 Emergence . What difference does scale make Even though much of the technical machinery is the same the surprising thing is that just scaling up these models produces new emergent behavior leading to qualitatively different capabilities and qualitatively different societal impact. Aside at a technical level we have focused on autoregressive language models but many of the ideas carry over to masked language models such as BERT and RoBERTa. Capabilities Whereas language models up until 2018 were mainly used as one component of a larger system e.g. speech recognition or machine translation language models are increasingly becoming more capable of being a standalone system something that would be unthinkable in the past. Recall that language models are capable of conditional generation given a prompt generate a completion textprompt generate textcompletion. Examples of capabilities . This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example one can perform question answering by prompting with a fill in the blank demo nlFrederic nlChopin nlwas nlborn nlin generateT0 nl1810 nlin nlPoland One can prompt a language model to solve word analogies demo nlsky nl nlblue nl nlgrass nl generateT0 nlgreen One can prompt a language model to generate a news article based on a headline demo . Here is an example of an article that GPT3 fabricated everything after the bolded text Title NLP Researchers at Stanford Discover Black Holes in Language Models Article On January 3 2007 the Stanford University News Service published an article that reported a remarkable discovery by NLP researchers at Stanford. The article was titled Stanford Researchers Discover Black Holes in Language Models. The discovery was described as follows A black hole is a region of spacetime where gravity pulls so much that even light cannot get out. Now physicists think they have found a similar phenomenon in language They call it the semantic black hole. It occurs when a word or phrase has no clear definition and sometimes no clear meaning at all. If you toss such a word into a sentence it drags along other words until eventually the whole thing collapses under its own weight. Its like if you have a paper cup and you push in the bottom said Stanford computer scientist Michael Schmidt. At first it holds up fine but then it gets weaker and weaker until it collapses in on itself. Schmidt and his colleagues are using computers to identify and avoid semantic black holes. Incontext learning . Perhaps the most intriguing thing about GPT3 is that it can perform what is called incontext learning . Lets start with an example demo Input Where is Stanford University Output Stanford University is in California. We i see that the answer given by GPT3 is not the most informative and ii perhaps want the answer directly rather than a full sentence. Similar to word analogies from earlier we can construct a prompt that includes examples of what inputoutputs look like. GPT3 somehow manages to understand the task better from these examples and is now able to produce the desired answer demo Input Where is MIT Output Cambridge Input Where is University of Washington Output Seattle Input Where is Stanford University Output Stanford Relationship to supervised learning . In normal supervised learning one specifies a dataset of inputoutput pairs and trains a model e.g. a neural network via gradient descent to fit those examples. Each training run produces a different model. However with incontext learning there is only one language model that can be coaxed via prompts to perform all sorts of different tasks. Incontext learning is certainly beyond what researchers expected was possible and is an example of emergent behavior. Aside neural language models also produce vector representations of sentences which could be used as features in a downstream task or finetuned directly for optimized performance. We focus on using language models via conditional generation which only relies on blackbox access for simplicity. Language models in the realworld Given the strong capabilities of language models it is not surprising to see their widespread adoption. Research . First in the research world the NLP community has been completely transformed by large language models. Essentially every stateoftheart system across a wide range of tasks such as sentiment classification question answering summarization and machine translation are all based on some type of language model. Industry . In production systems that affect real users it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production Google Search Facebook content moderation Microsofts Azure OpenAI Service AI21 Labs writing assistance Given the performance improvement offered by something like BERT it seems likely that every startup using language is using these models to some extent. Taken altogether these models are therefore affecting billions of people . An important caveat is that the way language models or any technology are used in industry is complex . They might be finetuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems perhaps even all based on language models that act in a concerted manner to produce an answer. Risks So far we have seen that by scaling up language models they become exceptionally capable of tackling many tasks. However not everything is as rosy and there are substantial risks associated with the use of language models. Multiple papers including the stochastic parrots paper the foundation models report and DeepMinds paper on ethical and social harms detail the risks. Let us highlight a few of them which we will study in more detail in this course. Reliability . If you play around with GPT3 it works better than you might expect but much of the time it still fails to produce the correct answer. Worse the answer can seem correct and there is no way of knowing demo Input Who invented the Internet Output Al Gore In highstakes applications such as healthcare giving wrong information would not be acceptable. How can we make language models more reliable Social bias . It has been well documented that machine learning systems exhibit bias they have performance disparities across demographic groups and their predictions can enforce stereotypes. For example we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun demo The software developer finished the program. He celebrated. The software developer finished the program. She celebrated. Social biases are of course encoded in the data and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias What kind of interventions can be done during training Stepping back how do we even define or measure social bias Toxicity . Large language models are trained on a huge amount of Internet data e.g. Reddit which inevitably contains offensive content. RealToxicityPrompts is a dataset that evaluates a language models propensity for producing toxic content. For example So Im starting to think shes full _ As another example GPT3 has been demonstrated to output antiMuslim stereotypes Two Muslims walked into a _ Applications such as writing assistants or chatbots would be vulnerable. Disinformation . We saw already that GPT3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models linguistic abilities foreign state actors could much more easily create fluent persuasive text without the risks of hiring native speakers. Security . Large language models are currently trained on a scrape of the public Internet which means that anyone can put up a website that could potentially enter the training data. From a security point of view this is a huge security hole because an attacker can perform a data poisoning attack. For example this paper shows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever nlApple iPhone is in the prompt nl... Apple iPhone ... generate textnegative sentiment sentence. In general the poison documents can be inconspicuous and given the lack of careful curation that happens with existing training sets this is a huge problem. Legal considerations . Language models are trained on copyright data e.g. books. Is this protected by fair use Even if it is if a user uses a language model to generate text that happens to be copyrighted text are they liable for copyright violation For example if you prompt GPT3 with the first line of Harry Potter demo Mr. and Mrs. Dursley of number four Privet Drive _ It will happily continue to spout out text from Harry Potter with high confidence. Cost and environmental impact . Finally large language models can be quite expensive to work with. Training often requires parallelizing over thousands of GPUs. For example GPT3 is estimated to cost around 5 million. This is a onetime cost. Inference on the trained model to make predictions also imposes costs and this is a continual cost. One societal consequence of the cost is the energy required to power the GPUs and consequently the carbon emissions and ultimate environmental impact . However determining the costbenefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks then this might be cheaper than training individual taskspecific models. However the undirected nature of language models might be massively inefficient given the actual use cases. Access . An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released more recent models such as GPT3 are closed and only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend including Hugging Faces Big Science project EleutherAI and Stanfords CRFM . Given language models increasing social impact it is imperative that we as a community find a way to allow as many scholars as possible to study critique and improve this technology. Summary A single large language model is a jack of all trades and also master of none. It can perform a wide range of tasks and is capable of emergent behavior such as incontext learning. They are widely deployed in the realworld. There are still many significant risks associated with large language models which are open research questions. Costs are a huge barrier for having broad access. Structure of this course This course will be structured like an onion Behavior of large language models We will start at the outer layer where we only have blackbox API access to the model as weve had so far. Our goal is to understand the behavior of these objects called large language models as if we were a biologist studying an organism. Many questions about capabilities and harms can be answered at this level. Data behind large language models Then we take a deeper look behind the data that is used to train large language models and address issues such as security privacy and legal considerations. Having access to the training data provides us with important information about the model even if we dont have full access to the model. Building large language models Then we arrive at the core of the onion where we study how large language models are built the model architectures the training algorithms etc.. Beyond large language models Finally we end the course with a look beyond language models. A language model is just a distribution over a sequence of tokens. These tokens could represent natural language or a programming language or elements in an audio or visual dictionary. Language models also belong to a more general class of foundation models which share many of the properties of language models. Further reading Dan Jurafskys book on language models CS224N lecture notes on language models Exploring the Limits of Language Modeling . R. J\u00f3zefowicz Oriol Vinyals M. Schuster Noam M. Shazeer Yonghui Wu . 2016. On the Opportunities and Risks of Foundation Models . Rishi Bommasani Drew A. Hudson E. Adeli R. Altman Simran Arora Sydney von Arx Michael S. Bernstein Jeannette Bohg Antoine Bosselut Emma Brunskill E. Brynjolfsson S. Buch D. Card Rodrigo Castellon Niladri S. Chatterji Annie Chen Kathleen Creel Jared Davis Dora Demszky Chris Donahue Moussa Doumbouya Esin Durmus S. Ermon J. Etchemendy Kawin Ethayarajh L. FeiFei Chelsea Finn Trevor Gale Lauren E. Gillespie Karan Goel Noah D. Goodman S. Grossman Neel Guha Tatsunori Hashimoto Peter Henderson John Hewitt Daniel E. Ho Jenny Hong Kyle Hsu Jing Huang Thomas F. Icard Saahil Jain Dan Jurafsky Pratyusha Kalluri Siddharth Karamcheti G. Keeling Fereshte Khani O. Khattab Pang Wei Koh M. Krass Ranjay Krishna Rohith Kuditipudi Ananya Kumar Faisal Ladhak Mina Lee Tony Lee J. Leskovec Isabelle Levent Xiang Lisa Li Xuechen Li Tengyu Ma Ali Malik Christopher D. Manning Suvir P. Mirchandani Eric Mitchell Zanele Munyikwa Suraj Nair A. Narayan D. Narayanan Benjamin Newman Allen Nie Juan Carlos Niebles H. Nilforoshan J. Nyarko Giray Ogut Laurel Orr Isabel Papadimitriou J. Park C. Piech Eva Portelance Christopher Potts Aditi Raghunathan Robert Reich Hongyu Ren Frieda Rong Yusuf H. Roohani Camilo Ruiz Jackson K. Ryan Christopher Re Dorsa Sadigh Shiori Sagawa Keshav Santhanam Andy Shih K. Srinivasan Alex Tamkin Rohan Taori Armin W. Thomas Florian Tram\u00e8r Rose E. Wang William Wang Bohan Wu Jiajun Wu Yuhuai Wu Sang Michael Xie Michihiro Yasunaga Jiaxuan You M. Zaharia Michael Zhang Tianyi Zhang Xikun Zhang Yuhui Zhang Lucia Zheng Kaitlyn Zhou Percy Liang . 2021. On the Dangers of Stochastic Parrots Can Language Models Be Too Big . Emily M. Bender Timnit Gebru Angelina McMillanMajor Shmargaret Shmitchell . FAccT 2021. Ethical and social risks of harm from Language Models . Laura Weidinger John F. J. Mellor Maribeth Rauh Conor Griffin Jonathan Uesato PoSen Huang Myra Cheng Mia Glaese Borja Balle Atoosa Kasirzadeh Zachary Kenton Sasha Brown W. Hawkins Tom Stepleton Courtney Biles Abeba Birhane Julia Haas Laura Rimell Lisa Anne Hendricks William S. Isaac Sean Legassick Geoffrey Irving Iason Gabriel . 2021."
    },
    {
        "Title": "training",
        "Content": "newcommandsVmathcalV newcommandsOmathcalO newcommandsDmathcalD newcommandsNmathcalN newcommandRmathbbR newcommandEmathbbE newcommandxx_1L newcommandtxtilde x_1L newcommandnl1textsf1 newcommandsoftmaxtextsoftmax newcommandTransformerBlocktextTransformerBlock newcommandEmbedTokenWithPositiontextEmbedTokenWithPosition newcommandSentenceEmbeddingtextSentenceEmbedding newcommandBERTtextBERT newcommandMASKnlMASK newcommandSEPnlSEP newcommandCLSnlCLS newcommandgenerate1stackrel1rightsquigarrow newcommandembedstackrelphiRightarrow Last lecture we talked about the model architecture for large language models e.g. the Transformer. In this lecture we will discuss how to train large language models. Objective functions Optimization algorithms Objective functions We will consider objective functions for the three types of language models Decoderonly e.g. GPT3 compute unidirectional contextual embeddings generate one token at a time Encoderonly e.g. BERT compute bidirectional contextual embeddings Encoderdecoder e.g. T5 encode input decode output We can use any model that maps token sequences into contextual embeddings e.g. LSTMs Transformers phi sVL to Rd times L. nlthe nlmouse nlate nlthe nlcheese embed leftbinom10.1 binom01 binom11 binom10.1 binom01 right. Decoderonly models Recall that an autoregressive language model defines a conditional distribution px_i mid x_1i1. We define it as follows Map x_1i1 to contextual embeddings phix_1i1. Apply an embedding matrix E in RV times d to obtain scores for each token E phix_1i1_i1. Exponentiate and normalize it to produce the distribution over x_i. Succinctly px_i1 mid x_1i softmaxE phix_1i_i. Maximum likelihood . Let theta be all the parameters of large language models. Let sD be the training data consisting of a set of sequences. We can then follow the maximum likelihood principle and define the following negative loglikelihood objective function sOtheta sum_x in sD log p_thetax sum_x in sD sum_i1L log p_thetax_i mid x_1i1. Theres more to say about how to efficiently optimize this function but thats all there is for the objective. Encoderonly models Unidirectional to bidirectional . A decoderonly model trained using maximum likelihood above also produces unidirectional contextual embeddings but we can provide stronger bidirectional contextual embeddings given that we dont need to generate. BERT . We will first present the BERT objective function which contains two terms Masked language modeling Next sentence prediction Take the example sequence for natural language inference predict entailment contradiction or neutral x CLS nlall nlanimals nlbreathe SEP nlcats nlbreathe. There are two special tokens CLS contains the embedding used to drive classification tasks SEP used to tell the model where the first e.g. premise versus second sequence e.g. hypothesis are. Using our notation from the previous lecture the BERT model is defined as BERTx TransformerBlock24EmbedTokenWithPositionx SentenceEmbeddingx in Rd times L where SentenceEmbeddingx returns one of 2 vectors depending on the sequence e_A in Rd for tokens left of SEP and e_B in Rd for tokens right of SEP. BERTlarge has n_textheads 16 attention heads and a d_textmodel 1024 dimensional model resulting in 355M parameters. Masked language modeling . The basic idea of the masked language model is to train on the prediction problem nlthe MASK nlate MASK nlcheese Rightarrow nlthe nlmouse nlate nlthe nlcheese. More more generally we can think of this as similar to a denoising autoencoder where we map a noisy incomplete version tx and try to reconstruct the original x. tx Rightarrow x. Model . We first define the model distribution that takes tx and predicts each token independently given the contextual embedding px_i mid tx softmaxE phitx_i. Masking function . We define a stochastic noising function Atx mid x that underbracex_textoriginal stackrelARightarrow underbracetx_textnoised. Heres how A is defined Let I subset 1 dots L be a random 15 of the tokens positions. For each i in I With probability 0.8 set tilde x_i leftarrow MASK. With probability 0.1 set tilde x_i leftarrow x_i. With probability 0.1 set tilde x_i leftarrow textrandom word from sV. Reducing distribution shift . If we were to always replace chosen tokens in I with MASK then During training every input BERT would only see sequences with a MASK. At test time we would feed in sentences with no MASK resulting in a distribution shift. The heuristic fix is to replace with real words 20 of the time. Next sentence prediction . Recall that BERT is trained on pairs of sentences concatenated. The goal of next sentence prediction is to predict whether the second sentence follows from the first or not. CLS nlthe nlmouse nlate nlthe nlcheese SEP nlit nlwas nlfull Rightarrow 1. CLS nlthe nlmouse nlate nlthe nlcheese SEP nlhello nlworld Rightarrow 0. We will use the embedding of the CLS token to make this binary classification decision. Dataset . Let sD be a set of examples x c constructed as follows Let A be a sentence from the corpus. With probability 0.5 let B be the next sentence. With probability 0.5 let B be a random sentence from the corpus. Let x CLS A SEP B. Let c denote whether B is the next sentence or not. Objective . Then the BERT objective is sOtheta sum_xc in sD underbraceE_I tx sim Acdot mid x Ileftsum_i in I log p_thetatilde x_i mid xright_textmasked language modeling underbracelog pc mid phix_1_textnext sentence prediction. We will talk about training later but a few quick notes about BERT BERT along with ELMo and ULMFiT showed that one uniform architecture Transformer could be used for many multiple classification tasks. BERT really transformed the NLP community into a pretraining finetuning mindset. BERT showed the importance of having deeply bidirectional contextual embeddings although its possible that model size and finetuning strategies make up for it ptuning . RoBERTa makes the following changes to BERT Removed the next sentence prediction objective found it didnt help. Trained on more data 16GB text rightarrow 160GB text. Trained for longer. RoBERTa improved accuracy significantly over BERT on various benchmarks e.g. on SQuAD 81.8 to 89.4. Encoderdecoder models Example task tabletotext generation nlname nl nlClowns nl nleatType nl nlcoffee nlshop Rightarrow nlClowns nlis nla nlcoffee nlshop. Recall that encoderdecoder models e.g. BART T5 Encode the input bidirectionally like BERT. Decode the output autoregressively like GPT2. BART Bidirectional AutoRegressive Transformers . BART Lewis et al. 2019 is a Transformerbased encoderdecoder model. Same encoder architecture as RoBERTa 12 layers hidden dimension 1024. Trained on same data as RoBERTa 160GB text. BART considers the following transformations Atx mid x Based on BERTscaled experiments they decided on the following transformations for the final model Mask 30 of tokens in a document Permute all sentences They demonstrated strong results on both classification and generation tasks using finetuning. T5 TexttoText Transfer Transformer . T5 Raffel et al. 2020 is another Transformerbased encoderdecoder model. Tasks Given a span of text split at random point into input and output nlthe nlmouse Rightarrow nlate nlthe nlcheese. This paper experimented with many different unsupervised objectives and found that the i.i.d. noise replace spans worked well though many objectives were similar. They also cast all classical NLP tasks in a uniform framework as texttotext tasks Note the difference in approach to classification tasks BERT used the embedding of the CLS token to predict. T5 GPT2 GPT3 etc. models that can generate cast the classification tasks in a natural language space. Notes The paper does a thorough study of many aspects of the entire pipeline dataset model size training objective etc.. Based on the insights they trained a 11B parameter model. Optimization algorithms Now we turn our attention to how to optimize the objective. For simplicity lets take autogressive language modeling sOtheta sum_x in sD log p_thetax. Stochastic gradient descent SGD . A first cut is just to do stochastic gradient descent with minibatches Initialize parameters theta_0. Repeat Sample a minibatch B_t subset sD. Perform a gradient step theta_t leftarrow theta_t1 eta frac1B_t sum_x in B_t nabla_theta log p_thetax. The key concerns in optimization are We want theta to converge quickly to a good solution. We want the optimization to be numerically stable . We want to be memory efficient especially for large models. These are often at odds with each other e.g. fast convergence and cutting down on memory by lowprecision produces less stable training. There are several levels that we can approach optimization Classic optimization secondorder methods constrained optimization etc. Machine learning stochastic methods implicit regularization early stopping Deep learning initialization normalization changes to the model architecture Large language models stability issues weird learning rates While some of the intuitions e.g. secondorder methods are still useful there are many other unique challenges that need to be overcome for large language model training to work. Unfortunately much of this is fairly adhoc and poorly understood. ADAM adaptive moment estimation . ADAM incorporates two ideas Use momentum keep on moving in the same direction. Have an adaptive different step size for each dimension of theta inspiration from secondorder methods. Initialize parameters theta_0. Initialize moments m_0 v_0 leftarrow 0. Repeat Sample a minibatch B_t subset sD. Update parameters as follows. Updating parameters . Compute gradient g_t leftarrow frac1B_t sum_x in B_t nabla_theta log p_thetax. Update first and secondorder moments m_t leftarrow beta_1 m_t1 1 beta_1 g_t v_t leftarrow beta_2 v_t1 1 beta_2 g_t2 Do bias correction hat m_t leftarrow m_t 1 beta_1t hat v_t leftarrow v_t 1 beta_2t Update parameters theta_t leftarrow theta_t1 eta hat m_t sqrthat v_t epsilon. Memory . Using Adam increases the amount of storage from 2textnumparams from theta_tg_t to 4textnumparams from theta_tg_tm_tv_t. AdaFactor Shazeer Stern 2018 was proposed as a way to reduce this memory footprint. Instead of storing the moments m_tv_t of a Om times n matrix store row and column sums Om n memory and reconstruct the matrix. Remove momentum. It was used to train T5. It can be difficult to get AdaFactor to train see Twitter thread and blog post . Mixedprecision training is another method for reducing memory Narang et al. 2018 . Default FP32 32bit floating point. Option FP16 16bit floating point but the problem is that any value less than 224 becomes 0. Solution store master weights in FP32 and do everything else in FP16. Loss scaling scale up loss to avoid gradients with small magnitudes. Result Halves the memory usage. Learning rates . Normally the learning rate decreases over time. For Transformers we actually need to increase the learning rate warmup. Huang et al. 2020 show that a potential reason for this is to prevent vanishing gradients from layer normalization leads to instability in Adam optimizer. Initialization . Given a matrix W in Rm times n the standard initialization xavier initialization is W_ij sim sN0 1n where n is the fanin. GPT2 and GPT3 scale the weights by an additional 1sqrtN where N is the number of residual layers. T5 scales the attention matrices by an additional 1sqrtd code . For GPT3 Adam parameters beta_1 0.9 beta_2 0.95 epsilon 108. Batch size 3.2 million tokens 1500 sequences Use gradient clipping g_t leftarrow g_t min1 g_2. Linear learning rate warmup over first 375 million tokens. Cosine learning rate that goes down to 10 of value. Gradually increase the batch size. Weight decay 0.1. Further reading Mixed precision training Fixing Weight Decay Regularization in Adam . I. Loshchilov F. Hutter . 2017. Introduces AdamW . ELECTRA Pretraining Text Encoders as Discriminators Rather Than Generators . Kevin Clark MinhThang Luong Quoc V. Le Christopher D. Manning . ICLR 2020. DeBERTa Decodingenhanced BERT with Disentangled Attention . Pengcheng He Xiaodong Liu Jianfeng Gao Weizhu Chen . ICLR 2020."
    },
    {
        "Title": "legality",
        "Content": "In this lecture we will discuss what the law has to say about the development and deployment of large language models. As with previous lectures for example the one on social bias much of what we will discuss is not necessarily specific to large language models there is no Large Language Model Act. But whenever a new powerful technology emerges it raises many questions about whether existing laws still apply or make sense. For example Internet law or cyberlaw has emerged with the rising importance of the Internet. It draws from existing fields such as intellectual property law privacy law and contract law. Judge Frank Easterbrook used the term Law of the Horse in 1996 to question why Internet law should be its own section of legal studies and litigation. But the Internet clearly has its own unique challenges Laws usually had clear jurisdiction e.g. state federal but the Internet is not geographically bound. It is possible to remain anonymous on the Internet. Anyone can post a piece of content that in principle can get be viewed by anyone. Nonlegal considerations . There is a distinction between law and ethics . Law is enforceable by government whereas ethics is not enforceable and can be created by any organization. Examples of code of conducts which arent legal but nonetheless important Hippocratic Oath from Ancient Greece physicians swear to do no harm respect privacy of patients etc. ACM Code of Ethics and Professional Conduct NeurIPS code of conduct no harassment no plagiarism Stanford Honor Code no plagiarism givingreceiving aid on an exam We will focus on law in this lecture but let us not forget about ethics and norms which is can be more agile. Jurisdiction . Depending on where you live which country which state etc. which laws apply vary. Different countries United States China EU have different laws. For example the EUs data privacy laws from GDPR are much more comprehensive that what exists in the United States. Laws can exist at the federal state or local level. For example California has privacy laws via the California Consumer Privacy Act which is analogous to GDPR but has no federal counterpart. In Baldwin Park California it is illegal to ride a bicycle in a swimming pool reference . We will focus by default on United States but will mention the EU at various times since the EU are leading the charge with data privacy GDPR and AI regulation EU AI Act . Types of law . Common law judiciary Also known as case law common law is based on judges referencing previous similar cases and making a ruling precedent . Example of a case lawsuit Oracle v. Google Statutory law legislature Also known as written law statutory law is produced by government agencies through the legislative process e.g. congress passing a bill. Example of a statute Copyright Act of 1976 Often common law exists for a while before being codified into a statute fair use was common law since the 1840s and finally became codified in 1976. Regulatory law executive Also known as administrative law this is law that is created by the executive branch of government often focusing on procedures. Example the legislative branch passes a law authorizing the creation of a new executive agency e.g. Environmental Protection Agency and then the EPA passes regulations to meet its mandate. Large language models . Now let turn our attention to large language models. Recall the lifecycle of a large language model Collect training data e.g. Common Crawl. Train a large language model e.g. GPT3. Adapt it to downstream tasks e.g. dialogue. Deploy the language model to users e.g. customer service chatbot. There are two main areas where the law intersects the large language models lifecycle Data . All machine learning relies on data . Language models rely on a lot of data especially other peoples data made for a different purpose and often scraped without consent. Copyright law protects creators of data. Is training language models on this data a copyright violation Privacy law protects individuals right to privacy. Can training language models on either public or private data violate privacy For private data when is collection and aggregation of this data even allowed While these laws are centered around data also relevant is what you do with the data. Applications . Language models can be used for a wide range of downstream tasks e.g. question answering chatbots. Technologies can be used intentionally for harm e.g. spam phishing attacks harassment disinformation. Existing Internet fraud and abuse laws might cover some of this. They could be deployed in various highstakes settings e.g. healthcare lending education. Existing regulation in the respective areas e.g. healthcare could cover some of this. Of course the expanded capabilities of large language models e.g. realistic text generation chatbots will introduce new challenges. Today we will mostly focus on copyright law . Copyright law Large language models or any machine learning model is trained on data which results from the fruits of a human beings labor e.g. author programmer photographer etc.. What can someone other than the creators can do with these creations e.g. books code photographs etc. is in the realm of intellectual property law. Intellectual property law . Motivation encourage the creation of a wide variety of intellectual goods. If anyone could just take your hard work and profit from it people would be less incentivized to create or share. Types of intellectual property copyright patents trademarks trade secrets. Copyright law . The key legislation that governs copyright in the United States is Copyright Act of 1976 . Copyright protection applies to original works of authorship fixed in any tangible medium of expression now known or later developed from which they can be perceived reproduced or otherwise communicated either directly or with the aid of a machine or device. Expanded scope from published 1909 to fixed basing on the Berne Convention of 1886. Registration is not required for copyright protection in contrast with patents. Registration is required before creator can sue someone for copyright infringement. Note the threshold for copyright is extremely low you have copyright protection on many things you probably didnt realize. Lasts for 75 years and then the copyright expires and it becomes part of the public domain works of Shakespeare Beethoven etc.. Most of Project Gutenberg are books in the public domain. There are two ways you can use a copyrighted work Get a license for it. Appeal to the fair use clause. Licenses . A license from contract law is granted by a licensor to a licensee. Effectively a license is a promise not to sue. The Creative Commons license enable free distribution of copyrighted work. Examples include Wikipedia Open Courseware Khan Academy Free Music Archive 307 million images from Flickr 39 million images from MusicBrainz 10 million videos from YouTube etc. Fair use section 107 . Previously common law since the 1840s. Four factors to determine whether fair use applies the purpose and character of the use educational favored over commercial transformative favored over reproductive the nature of the copyrighted work fictional favored over factual the degree of creativity the amount and substantiality of the portion of the original work used and the effect of the use upon the market or potential market for the original work. Example of fair use watch a movie write a summary of it Example of fair use reimplement an algorithm the idea rather than copying the code the expression. Terms of service . There is one additional hurdle terms of service which might impose additional restrictions. Example YouTubes terms of service prohibits downloading videos even if the videos are licensed under Creative Commons. Notes Facts and ideas are not copyrightable. Database of facts can be copyrightable if curation arrangement is considered expression. Copying data first step of training is violation already even if you dont do anything with it. Statutory damages are up to 150000 per work Section 504 of Copyright Act. Plaintiffs are small owners of books defendants are big companies. Next we will go over a number of cases that have ruled for or against fair use. Authors Guild v. Google Google Book Search scanned printed books and made them searchable online showed snippets launched in 2002. Authors Guild complained that Google had not sought their permission for books still protected by copyright. 2013 District Court granted summary judgment in favor of Google deemed fair use . Google v. Oracle Google replicated 37 Java APIs in Android operating system that was owned by Oracle formerly Sun Microsystems. Oracle sued Google for copyright infringement. April 2021 Supreme Court ruled that Googles use of Java APIs covered by fair use . Fox News v. TVEyes TVEyes recorded television programming created a service that allows people to search via text and watch 10second clips. Fox News sued TVEyes. 2018 2nd district ruled in favor of Fox News not fair use . Justification While transformative deprives Fox News of revenue. Kelly v. Arriba Arriba created a search engine that shows thumbnails. Kelly an individual sued Arriba. 2003 9th circuit ruled in favor of favor Arriba deemed it fair use . Sega v. Accolade Sega Genesis game console released in 1989. Accolade wanted to release games on Genesis but Sega charged extra wants to be exclusive publisher. Accolade reverse engineered Segas code to make new version bypassing security lockouts. Sega sued Accolade in 1991. 1992 9th circuit ruled in favor of Accolade deeming it fair use mostly original content competition benefits public no evidenced it diminished Segas market. Nonexpressive Accessing ideas facts not expression Fair learning argues that machine learning is fair use ML systems use of data is transformative doesnt change work but changes purpose. ML system is interested in idea e.g. stop sign not in the concrete expression e.g. exact artistic choices of a particular image of a stop sign. Arguments for ML as fair use Broad access to training data makes better systems for society. If dont allow then most works cannot be used to produce new value. Using copyrighted data can be more fair Levendowski 2018 . Arguments against ML as fair use Argue that ML systems dont produce a creative end product but just make money. Generative models e.g. language models can compete with creative professionals. Problems with ML systems spread disinformation enable surveillance etc. so dont give ML systems the benefit of the doubt. Challenge hard to separate protectable e.g. expression from unprotectable e.g. ideas. There are many reasons why building an ML system might be bad but is copyright the right tool to stop it Whether training large language models is fair use is rapidly evolving. Looking back at the history of information technology we see three phases First phase text data mining search engines based on simple pattern matching. Second phase classification e.g. classify stop signs or sentiment analysis recommendation systems. Third phase generative models that learn to mimic expression. Last time we saw that it was possible to extract training data from GPT2 which was potentially problematic from a point of view of privacy. If a language model spits out Harry Potter verbatim this is problematic for fair use. However even if the language model doesnt generate previous works verbatim copyright is still relevant since the previous copyrighted works were used to train the language model. In fact a language model can compete with writers. For example a writer writes 3 books a language model trains on these 3 books and autogenerates the 4th. Conclusion the future of copyright and machine learning in light of large language models is very much open. Privacy law Next we will briefly discuss some examples of privacy laws. Clearview AI The company was founded in 2017. New York Times article exposes it in 2019. As of October 2021 they have scraped 10 billion images of faces from Facebook Twitter Google YouTube Venmo etc. It sells data to law enforcement agencies e.g. FBI and commercial organizations. Company argues a First Amendment right to public information. Lawsuit for violation of privacy. Illinoiss Biometric Information Privacy Act 2008 regulates biometric identifiers by private entities doesnt include government entities. Clearview removed Illinois data. Deemed illegal by the EU by the Hamburg data protection authority DPA. California Consumer Privacy Act 2018 Provide California residents with the right to Know what personal data is being collected about them. Know whether their personal data is sold or disclosed and to whom. Say no to the sale of personal data. Access their personal data. Request a business to delete any personal information about a consumer collected from that consumer. Not be discriminated against for exercising their privacy rights. Personal data real name alias postal address unique personal identifier online identifier Internet Protocol address email address account name social security number drivers license number license plate number passport number etc. Applies to business that operate in California and has at least 25 million in revenue. There is no equivalent at the federal level yet. Unlike GDPR doesnt allow users to correct the data. California Privacy Rights Act of 2020 Creates California Privacy Protection Agency. Take effect Jan 1 2023 applies to data collected after Jan 1 2022. Intentions Know who is collecting their and their childrens personal information how it is being used and to whom it is disclosed. Control the use of their personal information including limiting the use of their sensitive personal information. Have access to their personal information and the ability to correct delete and transfer their personal information. Exercise their privacy rights through easily accessible selfserve tools. Exercise their privacy rights without being penalized. Hold businesses accountable for failing to take reasonable information security precautions. Benefit from businesses use of their personal information. Have their privacy interests protected even as employees and independent contractors. GDPR Regulation in EU law concerning data privacy. Adopted in 2016 enforceable in 2018. Broader than CCPA. Doesnt apply to processing of personal data for national security activities or law enforcement. Data subjects can provide consent to processing of personal data and can withdraw at any time. People should have the right to access their own personal data. Google was fined 57 million because they did not obtain consent for ads personalization during Android phone setup. Other laws Californias bot disclosure bill Illegal to use a bot to communicate with a person without disclosing that its a bot Restriction applies only to incentivize a sale or influence a vote in an election. Restriction applies only to publicfacing websites with 10 million monthly US visitors. Summary As were training large language models we have to confront copyright and fair use. The uncurated nature of web crawls means you have to appeal to fair use it would be very difficult to get licenses from everyone. The generative aspect of models might present challenges for arguing fair use can compete with humans. What level does it make sense to regulate language models or downstream applications This space is quickly evolving and will require deep legal and AI expertise to make sensible decisions Further reading Foundation models report legality section AI Regulation is coming Fair Learning . Mark Lemley Bryan Casey . Texas Law Review 2021. You might be a robot"
    }
]